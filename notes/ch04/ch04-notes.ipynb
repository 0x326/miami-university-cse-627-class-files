{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\def\\features{\\boldsymbol{\\phi}}$\n",
    "$\\def\\inputs{\\mathbf{x}}$\n",
    "$\\def\\targets{\\mathbf{t}}$\n",
    "$\\def\\weights{\\mathbf{w}}$\n",
    "$\\def\\x{\\inputs}$\n",
    "$\\def\\t{\\targets}$\n",
    "$\\def\\X{\\mathbf{X}}$\n",
    "$\\def\\w{\\weights}$\n",
    "$\\def\\normal{\\mathcal{N}}$\n",
    "$\\def\\mub{\\boldsymbol{\\mu}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Linear Classification\n",
    "- Deals with cases where the _target_ is _discrete_ (nominal). \n",
    "- Chapter 2 dealt with estimating the _likelihood_ $p(\\inputs | \\theta)$ and the _priors_ $p(\\theta)$. \n",
    "- Uses _Bayes Theorem_  in order to determine the least risky (most probable) class\n",
    "  $$ \\begin{align} \n",
    "  p(C_k|\\inputs) &= \\frac{p(\\inputs|C_k)p(C_k)}{p(\\inputs)}\\\\\n",
    "                 &= \\frac{p(\\inputs|C_k)p(C_k)}{\\sum_j p(\\inputs|C_j)p(C_j)}\\\\\n",
    "  \\end{align}$$\n",
    "   In a more general setting, minimizes the _expected loss_ which is dermined from $p(C_k|\\inputs)$.\n",
    "- Since **addition** is more robust then **multiplication**, we prefer to work with logarithms rather than raw probabilities. \n",
    "- Bayes theorem then becomes the _**softmax**_ function:\n",
    "  $$ p(C_k|\\inputs) = \\frac{\\exp\\{a_k\\}}{\\sum_j \\exp\\{a_j\\}},$$\n",
    "  where $a_k=\\ln\\{p(\\inputs|C_k)p(C_k)\\}$. \n",
    "  This is just rephrasing _Bayes_ theorem so we can focus on the form of $a_k$ in log space. \n",
    "- **Linear** classification problems have $a_k$ a linear function of $\\features(\\inputs)$. This means that, for _some_ choice of $\\weights$, we have $a_k = \\weights^T\\features(\\inputs)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1  Discriminant Functions\n",
    "- Sometimes we *do not care* about the _probabilities_, only the ultimate _decision_ that would result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.1  Two Classes ( Binary)\n",
    "- A binary linear classifier _partitions the input_ into regions separated by a **hyperplane**\n",
    "- In 2D, we are solving for a line (1D hyperplane)\n",
    "- In N-Dimensions, we split along an N-1 dimensional hyperplane\n",
    "- The missing dimension is **normal** (perpendicular) to the hyperplane. It is the weight vector $\\weights$!\n",
    "   - The $\\weights$ vector points toward the 'positive', $y(\\inputs) >0,$ side of the hyperplane\n",
    "   - The _signed distance_ is $y(\\weights)$ is propto the shortest distance from the hyperplane (blue dashed line)\n",
    "   - If $||\\weights||$ is large, then it will look as if the data is farther from the line...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figs/Figure4.1.png\" width=300 alt=\"Figure4.1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Homogeneous Coordinate Trick \n",
    "> *NOTE:* This should be very familiar to the computer graphics students\n",
    "\n",
    "**This explanation may or may not be confusing, skip it if needed. I find it helpful**<br>\n",
    "In **linear algebra**, vectors and matrices define subspaces. For example, a hyperplane (such as a line) is the _null space_ of a vector. However, subspaces **always include the origin**.  When finding a hyperplane (e.g. a line in 2D) that does not pass through the origin, it is convenient to define the plane using a single vector $\\tilde\\weights$ that is _one dimension higher than the input_. We do this by adding an extra coordinate to each input so $\\tilde\\inputs_n = (x_0, \\inputs_n)$ and $\\tilde\\weights = (1, \\weights_n)$.  Our 2D data is in a 3D space at the plane $x_0==1.$  This is a mathematical trick that allows us to use the tools of linear algebra and simplifies notation. *In the future sections of the book, we often omit the ~'s above $\\tilde\\weights$ and $\\tilde\\inputs$, but they should be understood to be there unless $w_0$ is called out separately. *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figs/my-plane-line-drawing.png\">  In this case ther is an $D+1$ Dimensional weioght vector $\\weights$ that is perpendicular to an $D$ dimensional hyperplane, and the intersection of that plane with $x_0=1$ is a $(D-1)$ dimensional hyperplane that does _not_ have to pass through the origin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The _intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperplane that separates data is set of points that solve $\\tilde\\weights^T\\tilde\\inputs=0$, or \n",
    "$\\weights^T\\inputs + w_0 = 0$. In this case the extra coordinate $w_0$ is called an _intercept_ or a _bias_ term, even though that is just plain confusing since bias means something else important in machine learning (e.g. the bias variance trade off). You will have to try to keep things clear based no context I am afraid. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **Half-space** is the portion of a space on _one side_ of a separating hyperplane. The _positive_ half space is where $\\tilde\\weights^T\\tilde\\inputs > 0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:** If you need a refresher I reccomend [watching Dr Strang](https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/) (the cool one that knows math, not magic). <br>\n",
    "<a href=\"https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/lecture-1-the-geometry-of-linear-equations\"><img src=\"https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/lecture-1-the-geometry-of-linear-equations/18.06_L01.jpg\">\n",
    "</a> <a href=\"https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/lecture-10-the-four-fundamental-subspaces\"><img src=\"https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/lecture-10-the-four-fundamental-subspaces/18.06_L10.jpg\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.2 Multiple Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key takeaway from this section is that it is not straightforward how to design a discriminant function that works. A couple of approaches are discussed:\n",
    "\n",
    "#### One-vs-Rest\n",
    "\n",
    "In this scenario we create $K-1$ binary classifiers. Each classifier decides whether it is calss $K$ or not. However, this can result in ambiguous regions that are assigned to multiple classes as shown below:\n",
    "<img src=\"figs/Figure4.2a.png\", width=\"200\"/>\n",
    "In the figure above the green region is prediected as __both__ class 1 and class 2. I'm sure we could come up with some heirarchical system to resolve the ambiguity, but the result would not be a pure linear classifier, and this type of issue is something to be aware of. \n",
    "\n",
    "#### One-vs-One\n",
    "Every pair of classes (if there are $K$ classes, how many pairs of classes are there?) is treated as a binary decision problem. When this works, class regions become convex cells that are the intersection of $K-1$ half-spaces.  Unfortunately, this can also result in regions that have no class label assigned to them.\n",
    "\n",
    "<img src=\"figs/Figure4.2b.png\", width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case (above), 3 classes require $\\frac{3\\times2}{2}=3$ separating hyperplanes (lines). The <font color=\"green\">**green**</font> regions are negative for all classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Argmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE: * I am calling this argmax, your book did not give this category a name. \n",
    "\n",
    "\n",
    "Instead of treeting the problem of determining if you are in class $k$ as a _binary_ $0/1$ decision problem, consider the _signed distance_ $y_k(\\inputs_n)$ from _a_ plane associated with each classifier. Assign each sample to the class with the highest signed distance. \n",
    "\n",
    "These planes _**are not**_ separating planes any more; the separating planes are instead the hyperplanes where $y_k(\\inputs) == y_j(\\inputs)$ for some other $j$. The planes are illustrated a few cells below; notice how the separation is where the planes _**intersect**_ with each-other in the multiclass situation. \n",
    "\n",
    "\n",
    "Recall that the signed distance is\n",
    "$$y_k(\\inputs) = \\weights_k^T \\inputs +w_{k0} = \\tilde\\weights_k^T\\tilde\\inputs.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below illustrates the way these linear functions $y_k$ can partition the input space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import  LogisticRegression\n",
    "\n",
    "def plot_planes(means, size=100, cov= 0.1*eye(2)):\n",
    "    K = len(means)  \n",
    "    points = concatenate([multivariate_normal(mean=means[k],cov=cov, size=size) for k in range(K)])\n",
    "    labels = concatenate([[k]*size for k in range(K)])\n",
    "\n",
    "    clf = LogisticRegression().fit(points, labels)\n",
    "    w = clf.coef_\n",
    "    w0 = clf.intercept_\n",
    "\n",
    "    X1, X2 = mgrid[points[:,0].min():points[:,0].max():100j, points[:,1].min():points[:,1].max():100j]\n",
    "    @np.vectorize\n",
    "    def y(k, x1, x2):\n",
    "        return w[k].T.dot([x1, x2])  + w0[k]\n",
    "   \n",
    "    Zs = array([y(k, X1, X2) for k in range(K)])\n",
    "    Ts = Zs.argmax(0)\n",
    "    Z = Zs.max(0)\n",
    "\n",
    "    colors = cm.jet(linspace(0,1,K))\n",
    "    ax = subplot(121)\n",
    "    ax.imshow(colors[Ts.T], origin='lower', alpha=0.5,\n",
    "              extent=(points[:,0].min(), points[:,0].max(), points[:,1].min(),points[:,1].max()),\n",
    "              zorder=1)\n",
    "    ax.contour(X1, X2, Z, colors='k')\n",
    "    \n",
    "    ax.scatter(*points.T, color=colors[labels], edgecolor='k', zorder=2)\n",
    "        \n",
    "    ax = subplot(122, projection='3d')\n",
    "    ax.scatter3D(*points.T, zs=1, color=colors[labels])\n",
    "    ax.plot_surface(X1, X2, Z, facecolors=colors[Ts][1:, 1:], rstride=5, cstride=5)\n",
    "\n",
    "    ax.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = figure(figsize=(8,4))\n",
    "K = 3\n",
    "plot_planes([(cos(k*2*pi/K), sin(k*2*pi/K)) for k in range(K)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = figure(figsize=(8,4))\n",
    "K = 4\n",
    "plot_planes(column_stack([linspace(-1, 1, K), zeros(K)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4 Fisher's Linear Discriminant\n",
    "\n",
    "#### For binary classification\n",
    "Given that the following asusmptions hold:\n",
    "- You have a binary classification problem\n",
    "- The likelihoods are gaussian\n",
    "- All of the likelihoods share the same covariance matrix\n",
    "\n",
    "This method for classifying exploits the fact that in this situation we know a _linear_ classifier works. \n",
    "Furthermore, we know that the separating perpendicular to the vector that connects the two means $\\mub_1$ and $\\mub_2$. \n",
    "\n",
    "The Fisher method _projects_ all of the points on to the _line_ that is parallel to $\\mathbf{w} \\propto \\mub_2-\\mub1$\n",
    "\n",
    "This results in a **one dimensional** classification problem that is easy to solve analytically. (Not described in text) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.5 Relation to Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip this for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.6 Multiclass Fisher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2-ary projection a 1-D space (to a line) makes threshold selection simple, even by brute force. The K-ary projection to a $K-1$ dimensional space is not interesting to us right now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.7 The perceptron algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The book does not really point this out, but a perceptron is (related to) the fundamental unit of a **neural network**. It is often depicted graphically as in the image below, which I found on Google.\n",
    "\n",
    "Some observations: \n",
    "- The _inputs_ are represented as circles; the first input $x_0=1$. (According to our scheme, the first weight should be called $w_0$ in this picture...)\n",
    "- The weights that multiply inpputs are drawn on the edges\n",
    "- The weighted inputs are added (this is just a deptiction of $\\tilde\\w^T\\tilde\\x$)\n",
    "- The result of the sum is called the _activiation_ ($a$), and it is fed into a nonliner _activation function_. \n",
    "- In this figure (and in the book) the activation function is a step function; sigmoids and other activation funcions (ReLU's, TBD later) are often used in practice. \n",
    "- The output, $y$, represents a binary decision. \n",
    "\n",
    "<img src=\"http://ataspinar.com/wp-content/uploads/2016/11/perceptron_schematic_overview.png\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y(x)=f(\\w^T\\phi(\\x))$$\n",
    "$$\\phi_0(\\x)=1$$\n",
    "$$f(a) = \\begin{cases}+1, & a\\geq 0 \\\\ -1, & a<0. \\end{cases}$$\n",
    "$$t_n \\in {-1,1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Uses gradient descent to optimize a criterion function $E_P(w)$\n",
    "- The energy $E_P$ is designed so that there is _no penalty_ for a correctly labeled sample. \n",
    "- The penalty for a _mislabeled sample_ should not be a constant, or gradient descent bases optimization would fail. \n",
    "- We can choose a function $E_P$ to make optimization easier, in this case:\n",
    "    $$ E_P(w) = -\\sum_{n\\in\\mathcal{M}} \\w^T\\phi_n t_n$$ \n",
    "  where $\\mathcal{M}$ is the set of _mislabeled_ points. \n",
    "- This choice of the energy function pulls the line closer to points that lie on the wrong side of it. It is zero if the data is separated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figs/Figure4.7a.png\" width=200 align='left'/>\n",
    "<img src=\"figs/Figure4.7b.png\" width=200 align='left'/>\n",
    "<img src=\"figs/Figure4.7c.png\" width=200 align='left'/>\n",
    "<img src=\"figs/Figure4.7d.png\" width=200 align='left'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For gradient descent, we start with an initial set of weights $\\w^{(0)}$ and update them using the gradient $\\nabla E_P$ so that\n",
    "$$\\w^{(\\tau+1)} = w^{(\\tau)} - \\eta \\nabla E_P(\\w)$$\n",
    "$$\\nabla E_P(\\w) = -\\sum_{n\\in\\mathcal{M}} \\phi_n t_n $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use _stochastice gradient desent_, then we can update this one sample at a time so that\n",
    "$$\\w^{(\\tau+1)} = w^{(\\tau)} + \\eta \\phi_n t_n, \\hspace{3em} n\\in \\mathcal{M}$$\n",
    "\n",
    "\n",
    "(at least) two results are possible:\n",
    "- The classes are separable, and so $E_P(\\w) = 0$ and $\\nabla E_p=0$ because $\\mathcal{M}$ is empty. \n",
    "- The classes are _not_ separable, and $E_P(\\w) > 0$, in which case stochastic gradient descent may _never_ converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
