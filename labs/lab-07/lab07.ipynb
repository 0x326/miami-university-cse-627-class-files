{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 07 -- More NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals for the lab\n",
    "- Repeat the logistic regression exercise but add tensorflow logging\n",
    "- Save the model weights\n",
    "- Add a hidden layer to the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "data = input_data.read_data_sets(\"data/MNIST/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The %run command executes a python file as if it was a cell in the notebook. \n",
    "%run plot_images.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch, y_batch = data.train.next_batch(9)\n",
    "plot_images(x_batch, y_batch.argmax(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up a net to estimate/predict digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "num_inputs = 28*28\n",
    "num_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input placeholders for the data `x` and the expected (1hot) labels `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(tf.float32, (None, 28*28), name='x')\n",
    "    y = tf.placeholder(tf.float32, (None, 10), name='y')\n",
    "    ec = tf.argmax(y, 1, name='ec') #[e]xpected [c]lass label\n",
    "  \n",
    "    # When this node is run, it will save an image to a log file.\n",
    "    #  - I am not assigning a variable to the result;,\n",
    "    #    all of the summary nodes can be collected by inspecting the graph later on. \n",
    "    tf.summary.image(\"image\", tf.reshape(x, [-1, 28,28,1]), max_outputs=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In tensorflow, you can create nodes in the graph whose purpose is to save information to log files. The log files can be inspected using a tool called [TensorBoard](https://www.tensorflow.org/get_started/summaries_and_tensorboard). I hope to be able to demonstrate it later on in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we save data to the log file, the names become more important. We can group related nodes together using a 'name_scope' in order to make the log files easier to understand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('output'):\n",
    "    b = tf.Variable(tf.zeros(num_outputs), name='b')\n",
    "    W = tf.Variable(tf.truncated_normal([num_inputs, num_outputs]), name='W')\n",
    "    a = tf.add(tf.matmul(x, W), b, name='a')\n",
    "    z = tf.nn.softmax(a, name='z')\n",
    "    pc = tf.argmax(z, 1, name='pc')  # [p]redicted [c]lass label\n",
    "    \n",
    "    # Create nodes that will save histograms to the log file\n",
    "    #  -- histograms are good summaries of the (large) amount of data in the weights. \n",
    "    #  -- The let you spot common issues with gradient descent (stalling, diverging)\n",
    "    tf.summary.histogram(\"weights\", W)\n",
    "    tf.summary.histogram(\"biases\", b)\n",
    "    tf.summary.histogram(\"activations\", pc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The net is now complete for doing _inference_ or _estimation_. That is, you could use it (if the weights were set). Right now the weights are randomly set, the results will be random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "x_batch, y_batch = data.test.next_batch(9)\n",
    "predictions = sess.run(pc, {x:x_batch, y:y_batch})\n",
    "plot_images(x_batch, y_batch.argmax(1), predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the net for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy'):\n",
    "    correct = tf.equal(ec, pc, name='correct')  # for each sample, did we get it right? \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='acc')\n",
    "\n",
    "    # Create a node to save the accuracy to a log file, \n",
    "    # so that we can go back and see how it improves after training. \n",
    "    tf.summary.scalar('accuracy', accuracy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normally, we would need to break the test data up into mini-batches.\n",
    "# however, this set is small enough that we will send the entire \n",
    "# test set in as one large batch. \n",
    "print \"Accuracy:\", sess.run(accuracy, {x:data.test.images, y:data.test.labels})\n",
    "print \"Random chance is around 0.10 (10%)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the log/summary data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start training, let's look at the log data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE:* All log files will accumulate in the folder unless you delete them, or switch to a different log folder. If you are running this many times and adjusting settings, it is a good idea to think about how to choose differently named log folders for output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./log/run0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all of the 'summary' nodes from the graph and generate a single log entry from them. \n",
    "summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility to save summarys to the log directory. \n",
    "#  -- This will create a file, each time we add a summary to the writer it gets appended to a log file.\n",
    "summary_writer = tf.summary.FileWriter('./log/run0', graph=sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, log =  sess.run([accuracy, summary],\n",
    "                     {x:data.test.images, y:data.test.labels})\n",
    "summary_writer.add_summary(log, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train the net, we need to define an objective function to minimize. In this case we will use the cross-entropy loss because our output it a softmax classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipes import quote\n",
    "print \"Acuracy:\", acc\n",
    "print \n",
    "print \"Make sure the correct conda environment is active, then run:\"\n",
    "print \"    tensorboard --logdir={} \".format(quote(os.path.abspath('./log')))\n",
    "print \"and open\"\n",
    "print \"    http://localhost:6006/\"\n",
    "print \"using your web browser\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the graph for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=a)\n",
    "    loss = tf.reduce_mean(cross_entropy, name='loss')\n",
    "\n",
    "    # And define a node to log it\n",
    "    tf.summary.scalar('loss', loss);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('solver'):\n",
    "    solver = tf.train.AdamOptimizer()\n",
    "    optimize = solver.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_writer = tf.summary.FileWriter('./log/run1/', graph=sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The solver has some parameters that need to be initializes\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Training cycle\n",
    "batch_size=100\n",
    "training_epochs=20\n",
    "display_epoch=1\n",
    "\n",
    "try:\n",
    "    saver.restore(sess, './models/logreg.cpt')\n",
    "except:\n",
    "    print \"No model to restore\"\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_loss = 0.\n",
    "    total_batch = int(data.train.num_examples/batch_size)\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = data.train.next_batch(batch_size)\n",
    "       \n",
    "        _, loss_, summary_ = sess.run([optimize, loss, summary],\n",
    "                                    {x: batch_xs, y: batch_ys})\n",
    "        \n",
    "        global_iteration =  epoch * total_batch + i\n",
    "        summary_writer.add_summary(summary_, global_iteration)\n",
    "        \n",
    "        avg_loss += loss_ / total_batch\n",
    "        \n",
    "    \n",
    "    # Display logs per epoch step\n",
    "    if (epoch+1) % display_epoch == 0:\n",
    "        print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_loss)\n",
    "    \n",
    "    saver.save(sess, './models/logreg.cpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO / Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back and add a hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
