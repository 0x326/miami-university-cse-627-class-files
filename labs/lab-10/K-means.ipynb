{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = make_blobs(n_samples=500, n_features=2, centers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(X[:,0], X[:,1], c=Y, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these clusters are Gaussian with isotropic covariance matrices ($\\Sigma=\\sigma^2 I$) and thay all have the same variance. This is the assumption of K-means, but K-means is still often used to initilaize density estimation when this is not the case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the pseudocode for K-means:\n",
    "\n",
    "\n",
    "1. Let $K$ denote the number of clusters\n",
    "2. Let $X$ denote the set of $N$ data points, each is an $M$-dimensional point. \n",
    "3. Set $\\mu_1^{(0)}...\\mu_k^{(0)}$ to $K$ randomly selected points from $X$\n",
    "4. For $i=0, 1, 2, ...$ until convergence\n",
    "5. &nbsp;&nbsp;&nbsp;&nbsp; \n",
    "      Set $d_{n,j}^{(i+1)} = ||X_n - \\mu_j^{(i)}||^2$, the distance of each point to the nearest cluster center. \n",
    "6. &nbsp;&nbsp;&nbsp;&nbsp;  Set $c_n^{(i+1)}=\\displaystyle{\\arg\\min_j d_{n,j}^{(i+1)}}$\n",
    "7. &nbsp;&nbsp;&nbsp;&nbsp; \n",
    "      Set $\\mu_c^{(i+1)} = \\frac{\\sum_{c_n=c} X_n }{\\text{count}({c_n=c})}$\n",
    "      \n",
    "In order to test for convergence, you either test whether all $c_n^{(i+1)}=c_n^({i})$ or you can check whether $\\sum_{c} \\left(\\mu_c^{(i+1)} - \\mu_c^{(i)}\\right)^2  < K\\tau $ where $\\tau$ is some tolerance parameter.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first lab assignment is to implement k-means. My implementation is in `my_kmean` which I will load in the next cell. You can compare your results to mine. \n",
    "> NOTE: My implementation is not a perfect example of clear pythonic code, but you may look at it f you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i my_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(K, X, tolerance, mu_init=None, maxit=50):\n",
    "    \"\"\" Do up to maxit iterations.\n",
    "    Stop when the change is less than tolerance.\n",
    "    Use mu_init to intialize mus, if it is None then randomly choose points\n",
    "    Return cluster centetrs (mu), and cluster assignments (c)\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mus, clusters = my_kmeans(3, X)\n",
    "scatter(X[:,0], X[:,1], c=clusters)\n",
    "for k in range(len(mus)):\n",
    "    text(mus[k,0], mus[k,1], r'$\\mu_{}$'.format(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM-GMM: Expectation Maximization for Guassians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = make_classification(n_samples=100, \n",
    "                           n_features=2,\n",
    "                           n_redundant=0,\n",
    "                           n_informative=2,\n",
    "                           n_classes=2, \n",
    "                           n_clusters_per_class=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(X[:,0], X[:,1], c=Y, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EM GMM algorithm is similar to K-means, except that we use _soft_ memberships.\n",
    "The probability $$p(x) = \\sum_j w_j N(x | \\mu_j, \\Sigma_j)$$ and the weights $w_j$ can be interpreted as the probability that a sample $x$ is generated by cluster $j$. \n",
    "\n",
    "This means that for each sample we can estimate the probability that is came from the $j^{th}$ gaussian\n",
    "$$ \n",
    "c_{n,j} = \\frac{w_j N(X_n|\\mu_j, \\Sigma_j)} {\\sum_k w_k N(X_n|\\mu_k, \\Sigma_k) }\n",
    "$$\n",
    "instead of $c_n$. \n",
    "\n",
    "Then, using this information,  at each iteration we update the $w_j, \\mu_j, \\Sigma_j$ parameters based on the weights:\n",
    "\n",
    "$$ w'_j = $$\n",
    "$$ \\mu'_j = $$\n",
    "$$ \\sigma'_j = $$\n",
    "\n",
    "and so at each iteration we alternate between calculating $c_{n,j}$ and than choosing $w, \\mu, \\Sigma$ based on those. The process can stop with the chang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
